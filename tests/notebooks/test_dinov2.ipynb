{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d2fdbb-27b5-419c-b7c1-a00a4b1b0ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tests on dinov2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251772b-54d5-414f-a762-594946803a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbdfb9-1928-4b93-ac3e-54d7ce8f24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14d35d-aa91-4f62-ad40-ad1275fa28b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fa7b5-288c-4106-a487-7088eecb069e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2d656-893a-4089-bb4f-c90fcaff532d",
   "metadata": {},
   "source": [
    "## Checking torch cache dir\n",
    "Warning : make sure to set torch hub cache dir to reuse downloade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa21a9-99c7-4d91-bb55-1afc0f5458c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.hub.get_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5997de-a1f7-42f8-b188-6e6621d13317",
   "metadata": {},
   "source": [
    "## Setting torch cache dir for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ef011-42c9-4455-b3e4-dcd75b5a823d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "torch_hub_dir = dinov2_utils.set_torch_hub_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32803d4-bb44-4a44-a4c9-2ffec26f49c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download dinov2 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88707501-7db7-4690-ac1d-35e2afda56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc  # Garbage collector interface\n",
    "from tqdm import tqdm  # Import tqdm for progress bar functionality\n",
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "# List of model names to be downloaded\n",
    "model_names = [\n",
    "    'dinov2_vits14_reg',\n",
    "    'dinov2_vitb14_reg',\n",
    "    'dinov2_vitl14_reg',\n",
    "    # 'dinov2_vitg14_reg'  # Uncomment or add more model names as needed\n",
    "]\n",
    "download_models = False\n",
    "\n",
    "if download_models:        \n",
    "    # Iterate over the list of model names with a progress bar\n",
    "    for name in tqdm(model_names, desc=\"Downloading DINOv2 models\", unit=\"model\"):\n",
    "        dinov2_utils.download_and_clear_memory(name)\n",
    "    \n",
    "    print(\"All models are downloaded and cleared from active memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1673381-86bd-4807-b2a5-a83e80391aca",
   "metadata": {},
   "source": [
    "## Checking xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a9c79-feea-4ac9-8a62-bafbd5d1ad31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "XFORMERS_ENABLED, XFORMERS_AVAILABLE = dinov2_utils.check_xformers_status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15db02-4f2f-48e1-9006-5a9c3f432c91",
   "metadata": {},
   "source": [
    "## Loading a dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112254a-732a-40b7-bd12-9b627ab784bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'dinov2_vitl14_reg'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = torch.hub.load('facebookresearch/dinov2', model_name, skip_validation=True, pretrained=False).to(device)\n",
    "model = torch.hub.load('/gpfswork/rech/ads/commun/models/facebookresearch_dinov2_main/', model_name, source='local').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ccb55-9d6a-40b9-b558-c68e8a1145ac",
   "metadata": {},
   "source": [
    "### Copying last attention layer\n",
    "Keeping a copy that will be unchange of the layer we will change to not reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3d24e-f77f-4a79-b1e8-efba817502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Assuming the last attention layer is what you want to clone\n",
    "original_attn_layer = model.blocks[-1].attn\n",
    "\n",
    "# Create a deep copy of the attention layer\n",
    "cloned_attn_layer = copy.deepcopy(original_attn_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e182b-09e2-425b-b9a0-7cc6ed5154d5",
   "metadata": {},
   "source": [
    "### Model and last layer definition (methods and attributes) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73827c6b-33fe-4d33-a0cd-9ac615676695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "dinov2_utils.inspect_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e0310-5f57-417d-975c-7e0c830310f1",
   "metadata": {},
   "source": [
    "### Inspecting output of last attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc791f5-258c-4ada-91a6-ff2b6c143af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "dinov2_utils.test_attention_output(model, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108681af-0eb1-4c8c-a6ce-b06d0fbd6f53",
   "metadata": {},
   "source": [
    "### Modifying the MemEffAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d163696-b615-42c9-89dc-32a20bb7fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from xformers.ops import memory_efficient_attention, unbind\n",
    "\n",
    "# Retrieve the class of the current attention layer for accurate subclassing\n",
    "actual_attention_class = cloned_attn_layer.__class__\n",
    "class CustomMemEffAttention(actual_attention_class):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, proj_bias=True, attn_drop=0.0, proj_drop=0.0):\n",
    "        super(CustomMemEffAttention, self).__init__(\n",
    "            dim=dim, \n",
    "            num_heads=num_heads, \n",
    "            qkv_bias=qkv_bias, \n",
    "            proj_bias=proj_bias, \n",
    "            attn_drop=attn_drop, \n",
    "            proj_drop=proj_drop\n",
    "        )\n",
    "        #self.last_attention_map = None  # Attribute to store the last attention map\n",
    "    \n",
    "    def forward(self, x: Tensor, attn_bias=None, return_attn=True) -> Tensor:\n",
    "        if not XFORMERS_AVAILABLE:\n",
    "            if attn_bias is not None:\n",
    "                raise AssertionError(\"xFormers is required for using nested tensors\")\n",
    "            return super().forward(x)\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = torch.unbind(qkv, dim=2)\n",
    "\n",
    "        x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n",
    "\n",
    "        scale = 1.0 / q.shape[-1] ** 0.5 # Default value which is used in memory_efficient_attention\n",
    "        #scale = self.scale # Checked the value and it's the same\n",
    "        \n",
    "        # Scale queries\n",
    "        q = q * scale \n",
    "\n",
    "        # Transpose for matmul\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "        k = k.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "        v = v.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1))  # (B, num_heads, N, N)\n",
    "\n",
    "        # Apply attention bias if provided\n",
    "        if attn_bias is not None:\n",
    "            attn = attn + attn_bias\n",
    "\n",
    "        # Apply softmax to get attention probabilities\n",
    "        attn = F.softmax(attn, dim=-1)  # (B, num_heads, N, N)\n",
    "\n",
    "        self.last_attention_map = attn.detach()\n",
    "\n",
    "        x = x.reshape([B, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "#if return_attn:\n",
    "        #    print('got in final return')\n",
    "        #    return x, self.last_attention_map\n",
    "        #else:\n",
    "\n",
    "\n",
    "\n",
    "# Extracting parameters from the cloned attention layer\n",
    "num_heads = cloned_attn_layer.num_heads  # Number of attention heads\n",
    "dim = cloned_attn_layer.qkv.out_features // 3  # Dimension should be checked against how qkv is set up in the class\n",
    "\n",
    "# Bias in qkv and projection can be inferred from the existence and not None check\n",
    "qkv_bias = cloned_attn_layer.qkv.bias is not None\n",
    "proj_bias = cloned_attn_layer.proj.bias is not None\n",
    "\n",
    "# Dropout values; these should be checked if they exist and are applied in the cloned layer\n",
    "attn_drop = cloned_attn_layer.attn_drop.p if hasattr(cloned_attn_layer, 'attn_drop') else 0.0\n",
    "proj_drop = cloned_attn_layer.proj_drop.p if hasattr(cloned_attn_layer, 'proj_drop') else 0.0\n",
    "\n",
    "# Replace the existing attention layer in the last block\n",
    "model.blocks[-1].attn = CustomMemEffAttention(\n",
    "    dim=dim,\n",
    "    num_heads=num_heads,\n",
    "    qkv_bias=qkv_bias,\n",
    "    proj_bias=proj_bias,\n",
    "    attn_drop=attn_drop,\n",
    "    proj_drop=proj_drop\n",
    ").to(device) # Since we've replaced a module that contains parameters, ensure to move the parameters to the correct device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb67298-5383-4780-86e3-ad84d7ee04d7",
   "metadata": {},
   "source": [
    "### Dynamically Override the forward Method of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933da089-1bd7-4192-8ec9-190c6b5768b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "# Assuming `model` is an instance of DinoVisionTransformer\n",
    "model.forward = dinov2_utils.modified_forward(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7431a1-2202-46d8-9f90-aeef0d95785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dinov2_utils.dinov2_modifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4dc893-bed8-4a21-9086-8ad89c0fc9d4",
   "metadata": {},
   "source": [
    "## Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e137f-18f2-4b9c-aab4-b788abc7f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 518\n",
    "\n",
    "random_image = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "# Assuming you can modify how the model is called to include return_attn\n",
    "output, attentions = model(random_image, return_attn=True)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention Weights Shape:\", attentions[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997588e-ed82-4048-ad01-5f6a5835d18f",
   "metadata": {},
   "source": [
    "## Visualisation of attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19f4a9-5898-41f2-b965-f3759698cf40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from lynx_id.data.dataset import LynxDataset\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as AP\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from lynx_id.data.transformations_and_augmentations import transforms_dinov2, augments_dinov2\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_csv = Path('/gpfsscratch/rech/ads/commun/datasets/extracted/lynx_dataset_full.csv')\n",
    "dataset = LynxDataset(dataset_csv, mode='single', transform = transforms_dinov2, augmentation=augments_dinov2, probabilities=[0,0,1])  # Default mode\n",
    "\n",
    "\n",
    "\n",
    "# Get the first item from the dataset\n",
    "input, output = dataset[12]\n",
    "\n",
    "\n",
    "#input = np.array(Image.open(\"chat.png\").convert('RGB'))\n",
    "\n",
    "transformed_image = input['image']\n",
    "#transformed_image = input\n",
    "\n",
    "# Assuming 'model' is already defined and loaded elsewhere\n",
    "# Make sure the model is in evaluation modeÂµ\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings, attentions = model(transformed_image.unsqueeze(0).to(device), return_attn=True)  # Add batch dimension\n",
    "\n",
    "attentions = attentions[0] # Keep only the first block\n",
    "print(embeddings.shape)\n",
    "print(attentions.shape)\n",
    "nh = attentions.shape[1] # number of head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416669a-3343-44ed-b992-70bf62574aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = attentions[0, :, 0, 5:].reshape(nh, -1)\n",
    "\n",
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f5250-e3ff-4788-b678-a5fc1b15d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_patch_size = 14\n",
    "transformed_image.shape\n",
    "w_featmap = transformed_image.shape[-2] // model_patch_size\n",
    "h_featmap = transformed_image.shape[-1] // model_patch_size\n",
    "print(nh, w_featmap, h_featmap, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a2410-3ae3-480a-8751-61f5be939ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_attn = attentions > np.quantile(attentions.cpu().numpy(),0.9)\n",
    "attentions = (th_attn*attentions)/attentions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149e677-b9ce-411a-93b1-134a6b22a62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e2a30016-cde5-4c2a-85a4-6f8cbcb72a5d",
   "metadata": {},
   "source": [
    "threshold = 0.7\n",
    "# we keep only a certain percentage of the mass\n",
    "val, idx = torch.sort(attentions)\n",
    "val /= torch.sum(val, dim=1, keepdim=True)\n",
    "cumval = torch.cumsum(val, dim=1)\n",
    "th_attn = cumval > (1 - threshold)\n",
    "idx2 = torch.argsort(idx)\n",
    "for head in range(nh):\n",
    "    th_attn[head] = th_attn[head][idx2[head]]\n",
    "th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "# interpolate\n",
    "th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=model.patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "attentions = th_attn * attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2fb99-0c75-49aa-9e54-67baed4ca167",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = attentions.reshape(nh, w_featmap, h_featmap).float()\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=model.patch_size, mode=\"nearest\")[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0b8b4-9193-4b33-aa42-7ed3ab8207c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attentions(image, attentions, method='mean'):\n",
    "    \"\"\"\n",
    "    Plot the image with the attention map next to it, and below all attention heads in a grid with two columns.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Tensor of the input image.\n",
    "    - attentions: numpy array or tensor of shape (num_heads, seq_len, seq_len).\n",
    "    - method: 'mean' or 'max' to determine how to aggregate attention heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    #image = image.permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    # Ensure attentions is a numpy array\n",
    "    if torch.is_tensor(attentions):\n",
    "        attentions = attentions.cpu().numpy()\n",
    "\n",
    "    # Aggregate attention heads based on the method\n",
    "    if method == 'mean':\n",
    "        attention_avg = np.mean(attentions, axis=0)\n",
    "    elif method == 'max':\n",
    "        attention_avg = np.max(attentions, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'mean' or 'max'\")\n",
    "\n",
    "    # Determine number of heads\n",
    "    num_heads = attentions.shape[0]\n",
    "    num_cols = 2\n",
    "    num_rows = (num_heads + 1) // num_cols + ((num_heads + 1) % num_cols > 0)\n",
    "\n",
    "    # Plot the input image and the aggregated attention map\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Show image\n",
    "    axs[0, 0].imshow(image.squeeze().permute(1,2,0).cpu().numpy())  # Adjust for image dimensions and type\n",
    "    axs[0, 0].set_title('Input Image')\n",
    "    axs[0, 0].axis('off')  # Turn off axis\n",
    "\n",
    "    # Show aggregated attention map\n",
    "    attention_map = attention_avg\n",
    "    axs[0, 1].imshow(attention_map)\n",
    "    axs[0, 1].set_title(f'Attention Map ({method})')\n",
    "    axs[0, 1].axis('off')  # Turn off axis\n",
    "\n",
    "    # Plot each attention head in a grid\n",
    "    for i in range(num_heads):\n",
    "        row = (i + 2) // num_cols\n",
    "        col = (i + 2) % num_cols\n",
    "        axs[row, col].imshow(attentions[i,:,:], cmap='inferno')\n",
    "        axs[row, col].set_title(f'Head {i+1}')\n",
    "        axs[row, col].axis('off')  # Turn off axis\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_heads + 2, num_rows * num_cols):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        axs[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `transformed_image` is the input image tensor and `attentions` is the attention tensor\n",
    "print(type(transformed_image))\n",
    "plot_attentions(transformed_image, attentions, method='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded988cd-0bf6-4f92-a13e-2f23d3017103",
   "metadata": {},
   "source": [
    "## Execution performance of dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b56e1-d1cb-4445-afa2-67fc43f663f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def get_available_models(prefix, base_path=torch.hub.get_dir()):\n",
    "    if base_path is None:\n",
    "        base_path = os.path.join(torch.hub.get_dir(), 'checkpoints')\n",
    "    else:\n",
    "        base_path = os.path.expanduser(base_path)\n",
    "\n",
    "    available_models = []\n",
    "\n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith(prefix) and file.endswith('.pth'):\n",
    "                # Append the relative path of the model file\n",
    "                available_models.append(os.path.join(root, file))\n",
    "\n",
    "    # Remove the file extension and get unique model types\n",
    "    available_model_types = list(set([os.path.basename(f).replace('.pth', '') for f in available_models]))\n",
    "    return available_model_types\n",
    "\n",
    "# Call the function with a specific prefix and print available models\n",
    "model_prefix = \"dinov2_\"  # Change this to the desired prefix\n",
    "available_dinov2_models = get_available_models(model_prefix)\n",
    "print(f\"Available models with prefix '{model_prefix}':\", available_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e894e4-50da-4436-9280-154baa30a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac52a7-4ef2-4ac9-a8e3-b14a5191224b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78c83c-152a-45bd-86f0-b2b0f3d9d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Static dictionary mapping model names to file names\n",
    "model_name_to_file = {\n",
    "    'dinov2_vitg14_reg': 'dinov2_vitg14_reg4_pretrain',\n",
    "    'dinov2_vitl14_reg': 'dinov2_vitl14_reg4_pretrain',\n",
    "    'dinov2_vitb14_reg': 'dinov2_vitb14_reg4_pretrain',\n",
    "    'dinov2_vits14_reg': 'dinov2_vits14_reg4_pretrain',\n",
    "    'dinov2_vits14': 'dinov2_vits14_pretrain'\n",
    "}\n",
    "\n",
    "# Define the order of model sizes for sorting\n",
    "model_order = ['s', 'b', 'l', 'g']\n",
    "\n",
    "def get_available_models(prefix, base_path=torch.hub.get_dir()):\n",
    "    if base_path is None:\n",
    "        base_path = os.path.join(torch.hub.get_dir(), 'checkpoints')\n",
    "    else:\n",
    "        base_path = os.path.expanduser(base_path)\n",
    "\n",
    "    available_files = []\n",
    "\n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith(prefix) and file.endswith('.pth'):\n",
    "                # Append the relative path of the model file\n",
    "                available_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Remove the file extension and get unique model types\n",
    "    available_model_files = list(set([os.path.basename(f).replace('.pth', '') for f in available_files]))\n",
    "    return available_model_files\n",
    "\n",
    "# Function to load models\n",
    "def load_model(model_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = torch.hub.load('facebookresearch/dinov2', model_name).to(device)\n",
    "    return model\n",
    "\n",
    "# Function to measure inference metrics\n",
    "def measure_inference_metrics(model, input_tensor):\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    peak_mem = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return inference_time, output.shape, (peak_mem - start_mem) / (1024 ** 2)  # MB\n",
    "\n",
    "# Function to measure training metrics\n",
    "def measure_training_metrics(model, input_tensor):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Dummy loss function for example\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, torch.randint(0, 1000, (input_tensor.size(0),)).to(input_tensor.device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    peak_mem = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    train_time = end_time - start_time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_time, (peak_mem - start_mem) / (1024 ** 2)  # MB\n",
    "\n",
    "# Main function to compare models\n",
    "def compare_models(prefix):\n",
    "    available_model_files = get_available_models(prefix)\n",
    "    \n",
    "    # Sort available models based on predefined order\n",
    "    sorted_models = sorted(\n",
    "        available_model_files, \n",
    "        key=lambda x: model_order.index(x.split('_')[1][-1]) if x.split('_')[1][-1] in model_order else -1\n",
    "    )\n",
    "    print(available_model_files)\n",
    "    print(sorted_models)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
    "\n",
    "    results = {}\n",
    "    for model_file in sorted_models:\n",
    "        # Find corresponding model name using the dictionary\n",
    "        model_name = next((key for key, value in model_name_to_file.items() if value == model_file), None)\n",
    "        if not model_name:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        print(f\"Measuring inference metrics for {model_name}...\")\n",
    "        inference_speed, output_shape, inference_peak_memory = measure_inference_metrics(model, input_tensor)\n",
    "        \n",
    "        print(f\"Measuring training metrics for {model_name}...\")\n",
    "        training_speed, training_peak_memory = measure_training_metrics(model, input_tensor)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'Inference': {\n",
    "                'Speed (s)': inference_speed,\n",
    "                'Output Shape': output_shape,\n",
    "                'Peak Memory Usage (MB)': inference_peak_memory\n",
    "            },\n",
    "            'Training': {\n",
    "                'Speed (s)': training_speed,\n",
    "                'Peak Memory Usage (MB)': training_peak_memory\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(f\"Inference - Speed: {inference_speed:.6f} seconds, Peak Memory: {inference_peak_memory:.2f} MB\")\n",
    "        print(f\"Training - Speed: {training_speed:.6f} seconds, Peak Memory: {training_peak_memory:.2f} MB\\n\")\n",
    "\n",
    "    print(\"All measurements completed.\")\n",
    "    return results\n",
    "\n",
    "# Call the main function to compare models\n",
    "model_prefix = \"dinov2_\"\n",
    "results = compare_models(model_prefix)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2cdbb-a344-45ec-814b-02de7f838364",
   "metadata": {},
   "source": [
    "## Training tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f32534c-e390-4fb9-ae89-4ce3b60763f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebb1f08-5f9d-4da1-b426-506aafe9cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    train_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_train.csv',\n",
    "    val_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_val.csv',\n",
    "    test_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_test.csv',\n",
    "    model_embedder_weights='/lustre/fswork/projects/rech/ads/commun/models/resnet50/pretrained_weights.pt',\n",
    "    triplet_precompute_save_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full/triplet_precompute.npz',\n",
    "    triplet_precompute_load_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full/triplet_precompute.npz',\n",
    "    experiment_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full_3layers',\n",
    "    device='cuda',  # or 'cpu', 'auto'\n",
    "    verbose=True,\n",
    "    epochs=10,\n",
    "    debug=False,\n",
    "    model_type=\"megadescriptor\",\n",
    "    eval_before_training=False,\n",
    "    add_dense_layers=True,\n",
    "    num_dense_layers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea102f38-58c5-458a-99fa-6fac607047ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running train_triplets with arguments: Namespace(train_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_train.csv', val_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_val.csv', test_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_test.csv', model_embedder_weights='/lustre/fswork/projects/rech/ads/commun/models/resnet50/pretrained_weights.pt', triplet_precompute_save_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full/triplet_precompute.npz', triplet_precompute_load_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full/triplet_precompute.npz', experiment_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full_3layers', device='cuda', verbose=True, epochs=10, debug=False, model_type='megadescriptor', eval_before_training=False, add_dense_layers=True, num_dense_layers=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torchvision-0.16.1+fdea156-py3.11-linux-x86_64.egg/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torchvision-0.16.1+fdea156-py3.11-linux-x86_64.egg/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "100%|##########| 2079/2079 [00:00<00:00, 5081560.62it/s]\n",
      "100%|##########| 2547/2547 [00:00<00:00, 5409059.39it/s]\n",
      "/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /gpfs7kro/gpfslocalsup/src/pub/anaconda-py3/2023.09/pytorch-gpu-2.1.1+py3.11.5+cuda-11.8/pytorch-2.1.1/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|##########| 1090/1090 [15:39<00:00,  1.16it/s, loss=0.257]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.05554155677283576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:27<00:00,  7.37it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.30303030303030304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|##########| 1090/1090 [15:37<00:00,  1.16it/s, loss=0.09]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.03862021868928857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:28<00:00,  7.36it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:36<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3241943241943242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|##########| 1090/1090 [15:34<00:00,  1.17it/s, loss=0.103]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.03366019608914305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:29<00:00,  7.29it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:38<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.33910533910533913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|##########| 1090/1090 [15:39<00:00,  1.16it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.02832290539774326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:27<00:00,  7.40it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.32756132756132755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|##########| 1090/1090 [15:28<00:00,  1.17it/s, loss=0.0573] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.026569484563869074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:27<00:00,  7.39it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3588263588263588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|##########| 1090/1090 [15:33<00:00,  1.17it/s, loss=0]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.02482793127451468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:28<00:00,  7.35it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.354978354978355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|##########| 1090/1090 [15:24<00:00,  1.18it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.021873615581303015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:29<00:00,  7.29it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3641173641173641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|##########| 1090/1090 [15:21<00:00,  1.18it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.02136812958558765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:31<00:00,  7.19it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3645983645983646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|##########| 1090/1090 [15:25<00:00,  1.18it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.020225019518012574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:29<00:00,  7.27it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:38<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3463203463203463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|##########| 1090/1090 [15:27<00:00,  1.17it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.018262770249072566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute embeddings: 100%|##########| 1090/1090 [02:30<00:00,  7.27it/s]\n",
      "Compute embeddings: 100%|##########| 260/260 [00:37<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Number of images: 8714 | Embedding shape: 1536\n",
      "VAL   | Number of images: 2079   | Embedding shape: 1536\n",
      "VAL | Accuracy 1-KNN: 0.3641173641173641\n",
      "Best model saved at: /lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full_3layers/2024-12-18_10-44-45/model_best_0.018.pth\n",
      "Last model saved at: /lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/megad_noswiss_full_3layers/2024-12-18_10-44-45/model_last_0.018.pth\n",
      "Training completed. Now, start of evaluation on the model of the last epoch.\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/xpython_1750797/17239407.py\", line 2, in <module>\n",
      "    train_dinov2.main(args)\n",
      "  File \"/lustre/fswork/projects/rech/ads/ssos023/DP-SCR_Identify-and-estimate-density-lynx-population/lynx_id/scripts/train/train_dinov2.py\", line 294, in main\n",
      "NameError: name 'val_eval_metrics' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from lynx_id.scripts.train import train_dinov2\n",
    "train_dinov2.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f50e73-c694-4229-a595-8d9a4964f7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aad8f-e5ad-40f8-a80d-a78bd075eebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.1.1_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

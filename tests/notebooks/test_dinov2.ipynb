{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d2fdbb-27b5-419c-b7c1-a00a4b1b0ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tests on dinov2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6251772b-54d5-414f-a762-594946803a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cbdfb9-1928-4b93-ac3e-54d7ce8f24de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 21 13:42:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             88W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14d35d-aa91-4f62-ad40-ad1275fa28b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330fa7b5-288c-4106-a487-7088eecb069e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2d656-893a-4089-bb4f-c90fcaff532d",
   "metadata": {},
   "source": [
    "## Checking torch cache dir\n",
    "Warning : make sure to set torch hub cache dir to reuse downloade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffa21a9-99c7-4d91-bb55-1afc0f5458c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/linkhome/idris/genidr/ssos023/.cache/torch/hub'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.get_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5997de-a1f7-42f8-b188-6e6621d13317",
   "metadata": {},
   "source": [
    "## Setting torch cache dir for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ef011-42c9-4455-b3e4-dcd75b5a823d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "torch_hub_dir = dinov2_utils.set_torch_hub_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32803d4-bb44-4a44-a4c9-2ffec26f49c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download dinov2 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88707501-7db7-4690-ac1d-35e2afda56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc  # Garbage collector interface\n",
    "from tqdm import tqdm  # Import tqdm for progress bar functionality\n",
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "# List of model names to be downloaded\n",
    "model_names = [\n",
    "    'dinov2_vits14_reg',\n",
    "    'dinov2_vitb14_reg',\n",
    "    'dinov2_vitl14_reg',\n",
    "    # 'dinov2_vitg14_reg'  # Uncomment or add more model names as needed\n",
    "]\n",
    "download_models = False\n",
    "\n",
    "if download_models:        \n",
    "    # Iterate over the list of model names with a progress bar\n",
    "    for name in tqdm(model_names, desc=\"Downloading DINOv2 models\", unit=\"model\"):\n",
    "        dinov2_utils.download_and_clear_memory(name)\n",
    "    \n",
    "    print(\"All models are downloaded and cleared from active memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1673381-86bd-4807-b2a5-a83e80391aca",
   "metadata": {},
   "source": [
    "## Checking xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a9c79-feea-4ac9-8a62-bafbd5d1ad31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "XFORMERS_ENABLED, XFORMERS_AVAILABLE = dinov2_utils.check_xformers_status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15db02-4f2f-48e1-9006-5a9c3f432c91",
   "metadata": {},
   "source": [
    "## Loading a dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112254a-732a-40b7-bd12-9b627ab784bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'dinov2_vitl14_reg'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = torch.hub.load('facebookresearch/dinov2', model_name, skip_validation=True, pretrained=False).to(device)\n",
    "model = torch.hub.load('/gpfswork/rech/ads/commun/models/facebookresearch_dinov2_main/', model_name, source='local').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ccb55-9d6a-40b9-b558-c68e8a1145ac",
   "metadata": {},
   "source": [
    "### Copying last attention layer\n",
    "Keeping a copy that will be unchange of the layer we will change to not reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3d24e-f77f-4a79-b1e8-efba817502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Assuming the last attention layer is what you want to clone\n",
    "original_attn_layer = model.blocks[-1].attn\n",
    "\n",
    "# Create a deep copy of the attention layer\n",
    "cloned_attn_layer = copy.deepcopy(original_attn_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e182b-09e2-425b-b9a0-7cc6ed5154d5",
   "metadata": {},
   "source": [
    "### Model and last layer definition (methods and attributes) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73827c6b-33fe-4d33-a0cd-9ac615676695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "dinov2_utils.inspect_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e0310-5f57-417d-975c-7e0c830310f1",
   "metadata": {},
   "source": [
    "### Inspecting output of last attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc791f5-258c-4ada-91a6-ff2b6c143af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "dinov2_utils.test_attention_output(model, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108681af-0eb1-4c8c-a6ce-b06d0fbd6f53",
   "metadata": {},
   "source": [
    "### Modifying the MemEffAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d163696-b615-42c9-89dc-32a20bb7fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from xformers.ops import memory_efficient_attention, unbind\n",
    "\n",
    "# Retrieve the class of the current attention layer for accurate subclassing\n",
    "actual_attention_class = cloned_attn_layer.__class__\n",
    "class CustomMemEffAttention(actual_attention_class):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, proj_bias=True, attn_drop=0.0, proj_drop=0.0):\n",
    "        super(CustomMemEffAttention, self).__init__(\n",
    "            dim=dim, \n",
    "            num_heads=num_heads, \n",
    "            qkv_bias=qkv_bias, \n",
    "            proj_bias=proj_bias, \n",
    "            attn_drop=attn_drop, \n",
    "            proj_drop=proj_drop\n",
    "        )\n",
    "        #self.last_attention_map = None  # Attribute to store the last attention map\n",
    "    \n",
    "    def forward(self, x: Tensor, attn_bias=None, return_attn=True) -> Tensor:\n",
    "        if not XFORMERS_AVAILABLE:\n",
    "            if attn_bias is not None:\n",
    "                raise AssertionError(\"xFormers is required for using nested tensors\")\n",
    "            return super().forward(x)\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = torch.unbind(qkv, dim=2)\n",
    "\n",
    "        x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n",
    "\n",
    "        scale = 1.0 / q.shape[-1] ** 0.5 # Default value which is used in memory_efficient_attention\n",
    "        #scale = self.scale # Checked the value and it's the same\n",
    "        \n",
    "        # Scale queries\n",
    "        q = q * scale \n",
    "\n",
    "        # Transpose for matmul\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "        k = k.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "        v = v.transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1))  # (B, num_heads, N, N)\n",
    "\n",
    "        # Apply attention bias if provided\n",
    "        if attn_bias is not None:\n",
    "            attn = attn + attn_bias\n",
    "\n",
    "        # Apply softmax to get attention probabilities\n",
    "        attn = F.softmax(attn, dim=-1)  # (B, num_heads, N, N)\n",
    "\n",
    "        self.last_attention_map = attn.detach()\n",
    "\n",
    "        x = x.reshape([B, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "#if return_attn:\n",
    "        #    print('got in final return')\n",
    "        #    return x, self.last_attention_map\n",
    "        #else:\n",
    "\n",
    "\n",
    "\n",
    "# Extracting parameters from the cloned attention layer\n",
    "num_heads = cloned_attn_layer.num_heads  # Number of attention heads\n",
    "dim = cloned_attn_layer.qkv.out_features // 3  # Dimension should be checked against how qkv is set up in the class\n",
    "\n",
    "# Bias in qkv and projection can be inferred from the existence and not None check\n",
    "qkv_bias = cloned_attn_layer.qkv.bias is not None\n",
    "proj_bias = cloned_attn_layer.proj.bias is not None\n",
    "\n",
    "# Dropout values; these should be checked if they exist and are applied in the cloned layer\n",
    "attn_drop = cloned_attn_layer.attn_drop.p if hasattr(cloned_attn_layer, 'attn_drop') else 0.0\n",
    "proj_drop = cloned_attn_layer.proj_drop.p if hasattr(cloned_attn_layer, 'proj_drop') else 0.0\n",
    "\n",
    "# Replace the existing attention layer in the last block\n",
    "model.blocks[-1].attn = CustomMemEffAttention(\n",
    "    dim=dim,\n",
    "    num_heads=num_heads,\n",
    "    qkv_bias=qkv_bias,\n",
    "    proj_bias=proj_bias,\n",
    "    attn_drop=attn_drop,\n",
    "    proj_drop=proj_drop\n",
    ").to(device) # Since we've replaced a module that contains parameters, ensure to move the parameters to the correct device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb67298-5383-4780-86e3-ad84d7ee04d7",
   "metadata": {},
   "source": [
    "### Dynamically Override the forward Method of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933da089-1bd7-4192-8ec9-190c6b5768b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynx_id.utils import dinov2_utils\n",
    "\n",
    "# Assuming `model` is an instance of DinoVisionTransformer\n",
    "model.forward = dinov2_utils.modified_forward(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7431a1-2202-46d8-9f90-aeef0d95785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dinov2_utils.dinov2_modifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4dc893-bed8-4a21-9086-8ad89c0fc9d4",
   "metadata": {},
   "source": [
    "## Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e137f-18f2-4b9c-aab4-b788abc7f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 518\n",
    "\n",
    "random_image = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "# Assuming you can modify how the model is called to include return_attn\n",
    "output, attentions = model(random_image, return_attn=True)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention Weights Shape:\", attentions[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997588e-ed82-4048-ad01-5f6a5835d18f",
   "metadata": {},
   "source": [
    "## Visualisation of attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19f4a9-5898-41f2-b965-f3759698cf40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from lynx_id.data.dataset import LynxDataset\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as AP\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from lynx_id.data.transformations_and_augmentations import transforms_dinov2, augments_dinov2\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_csv = Path('/gpfsscratch/rech/ads/commun/datasets/extracted/lynx_dataset_full.csv')\n",
    "dataset = LynxDataset(dataset_csv, mode='single', transform = transforms_dinov2, augmentation=augments_dinov2, probabilities=[0,0,1])  # Default mode\n",
    "\n",
    "\n",
    "\n",
    "# Get the first item from the dataset\n",
    "input, output = dataset[12]\n",
    "\n",
    "\n",
    "#input = np.array(Image.open(\"chat.png\").convert('RGB'))\n",
    "\n",
    "transformed_image = input['image']\n",
    "#transformed_image = input\n",
    "\n",
    "# Assuming 'model' is already defined and loaded elsewhere\n",
    "# Make sure the model is in evaluation modeµ\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings, attentions = model(transformed_image.unsqueeze(0).to(device), return_attn=True)  # Add batch dimension\n",
    "\n",
    "attentions = attentions[0] # Keep only the first block\n",
    "print(embeddings.shape)\n",
    "print(attentions.shape)\n",
    "nh = attentions.shape[1] # number of head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416669a-3343-44ed-b992-70bf62574aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = attentions[0, :, 0, 5:].reshape(nh, -1)\n",
    "\n",
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f5250-e3ff-4788-b678-a5fc1b15d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_patch_size = 14\n",
    "transformed_image.shape\n",
    "w_featmap = transformed_image.shape[-2] // model_patch_size\n",
    "h_featmap = transformed_image.shape[-1] // model_patch_size\n",
    "print(nh, w_featmap, h_featmap, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a2410-3ae3-480a-8751-61f5be939ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_attn = attentions > np.quantile(attentions.cpu().numpy(),0.9)\n",
    "attentions = (th_attn*attentions)/attentions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149e677-b9ce-411a-93b1-134a6b22a62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e2a30016-cde5-4c2a-85a4-6f8cbcb72a5d",
   "metadata": {},
   "source": [
    "threshold = 0.7\n",
    "# we keep only a certain percentage of the mass\n",
    "val, idx = torch.sort(attentions)\n",
    "val /= torch.sum(val, dim=1, keepdim=True)\n",
    "cumval = torch.cumsum(val, dim=1)\n",
    "th_attn = cumval > (1 - threshold)\n",
    "idx2 = torch.argsort(idx)\n",
    "for head in range(nh):\n",
    "    th_attn[head] = th_attn[head][idx2[head]]\n",
    "th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "# interpolate\n",
    "th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=model.patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "attentions = th_attn * attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2fb99-0c75-49aa-9e54-67baed4ca167",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = attentions.reshape(nh, w_featmap, h_featmap).float()\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=model.patch_size, mode=\"nearest\")[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0b8b4-9193-4b33-aa42-7ed3ab8207c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attentions(image, attentions, method='mean'):\n",
    "    \"\"\"\n",
    "    Plot the image with the attention map next to it, and below all attention heads in a grid with two columns.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Tensor of the input image.\n",
    "    - attentions: numpy array or tensor of shape (num_heads, seq_len, seq_len).\n",
    "    - method: 'mean' or 'max' to determine how to aggregate attention heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    #image = image.permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    # Ensure attentions is a numpy array\n",
    "    if torch.is_tensor(attentions):\n",
    "        attentions = attentions.cpu().numpy()\n",
    "\n",
    "    # Aggregate attention heads based on the method\n",
    "    if method == 'mean':\n",
    "        attention_avg = np.mean(attentions, axis=0)\n",
    "    elif method == 'max':\n",
    "        attention_avg = np.max(attentions, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'mean' or 'max'\")\n",
    "\n",
    "    # Determine number of heads\n",
    "    num_heads = attentions.shape[0]\n",
    "    num_cols = 2\n",
    "    num_rows = (num_heads + 1) // num_cols + ((num_heads + 1) % num_cols > 0)\n",
    "\n",
    "    # Plot the input image and the aggregated attention map\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Show image\n",
    "    axs[0, 0].imshow(image.squeeze().permute(1,2,0).cpu().numpy())  # Adjust for image dimensions and type\n",
    "    axs[0, 0].set_title('Input Image')\n",
    "    axs[0, 0].axis('off')  # Turn off axis\n",
    "\n",
    "    # Show aggregated attention map\n",
    "    attention_map = attention_avg\n",
    "    axs[0, 1].imshow(attention_map)\n",
    "    axs[0, 1].set_title(f'Attention Map ({method})')\n",
    "    axs[0, 1].axis('off')  # Turn off axis\n",
    "\n",
    "    # Plot each attention head in a grid\n",
    "    for i in range(num_heads):\n",
    "        row = (i + 2) // num_cols\n",
    "        col = (i + 2) % num_cols\n",
    "        axs[row, col].imshow(attentions[i,:,:], cmap='inferno')\n",
    "        axs[row, col].set_title(f'Head {i+1}')\n",
    "        axs[row, col].axis('off')  # Turn off axis\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_heads + 2, num_rows * num_cols):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        axs[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `transformed_image` is the input image tensor and `attentions` is the attention tensor\n",
    "print(type(transformed_image))\n",
    "plot_attentions(transformed_image, attentions, method='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded988cd-0bf6-4f92-a13e-2f23d3017103",
   "metadata": {},
   "source": [
    "## Execution performance of dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b56e1-d1cb-4445-afa2-67fc43f663f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def get_available_models(prefix, base_path=torch.hub.get_dir()):\n",
    "    if base_path is None:\n",
    "        base_path = os.path.join(torch.hub.get_dir(), 'checkpoints')\n",
    "    else:\n",
    "        base_path = os.path.expanduser(base_path)\n",
    "\n",
    "    available_models = []\n",
    "\n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith(prefix) and file.endswith('.pth'):\n",
    "                # Append the relative path of the model file\n",
    "                available_models.append(os.path.join(root, file))\n",
    "\n",
    "    # Remove the file extension and get unique model types\n",
    "    available_model_types = list(set([os.path.basename(f).replace('.pth', '') for f in available_models]))\n",
    "    return available_model_types\n",
    "\n",
    "# Call the function with a specific prefix and print available models\n",
    "model_prefix = \"dinov2_\"  # Change this to the desired prefix\n",
    "available_dinov2_models = get_available_models(model_prefix)\n",
    "print(f\"Available models with prefix '{model_prefix}':\", available_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e894e4-50da-4436-9280-154baa30a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac52a7-4ef2-4ac9-a8e3-b14a5191224b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78c83c-152a-45bd-86f0-b2b0f3d9d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Static dictionary mapping model names to file names\n",
    "model_name_to_file = {\n",
    "    'dinov2_vitg14_reg': 'dinov2_vitg14_reg4_pretrain',\n",
    "    'dinov2_vitl14_reg': 'dinov2_vitl14_reg4_pretrain',\n",
    "    'dinov2_vitb14_reg': 'dinov2_vitb14_reg4_pretrain',\n",
    "    'dinov2_vits14_reg': 'dinov2_vits14_reg4_pretrain',\n",
    "    'dinov2_vits14': 'dinov2_vits14_pretrain'\n",
    "}\n",
    "\n",
    "# Define the order of model sizes for sorting\n",
    "model_order = ['s', 'b', 'l', 'g']\n",
    "\n",
    "def get_available_models(prefix, base_path=torch.hub.get_dir()):\n",
    "    if base_path is None:\n",
    "        base_path = os.path.join(torch.hub.get_dir(), 'checkpoints')\n",
    "    else:\n",
    "        base_path = os.path.expanduser(base_path)\n",
    "\n",
    "    available_files = []\n",
    "\n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith(prefix) and file.endswith('.pth'):\n",
    "                # Append the relative path of the model file\n",
    "                available_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Remove the file extension and get unique model types\n",
    "    available_model_files = list(set([os.path.basename(f).replace('.pth', '') for f in available_files]))\n",
    "    return available_model_files\n",
    "\n",
    "# Function to load models\n",
    "def load_model(model_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = torch.hub.load('facebookresearch/dinov2', model_name).to(device)\n",
    "    return model\n",
    "\n",
    "# Function to measure inference metrics\n",
    "def measure_inference_metrics(model, input_tensor):\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    peak_mem = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return inference_time, output.shape, (peak_mem - start_mem) / (1024 ** 2)  # MB\n",
    "\n",
    "# Function to measure training metrics\n",
    "def measure_training_metrics(model, input_tensor):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Dummy loss function for example\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, torch.randint(0, 1000, (input_tensor.size(0),)).to(input_tensor.device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    peak_mem = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    train_time = end_time - start_time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_time, (peak_mem - start_mem) / (1024 ** 2)  # MB\n",
    "\n",
    "# Main function to compare models\n",
    "def compare_models(prefix):\n",
    "    available_model_files = get_available_models(prefix)\n",
    "    \n",
    "    # Sort available models based on predefined order\n",
    "    sorted_models = sorted(\n",
    "        available_model_files, \n",
    "        key=lambda x: model_order.index(x.split('_')[1][-1]) if x.split('_')[1][-1] in model_order else -1\n",
    "    )\n",
    "    print(available_model_files)\n",
    "    print(sorted_models)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
    "\n",
    "    results = {}\n",
    "    for model_file in sorted_models:\n",
    "        # Find corresponding model name using the dictionary\n",
    "        model_name = next((key for key, value in model_name_to_file.items() if value == model_file), None)\n",
    "        if not model_name:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        print(f\"Measuring inference metrics for {model_name}...\")\n",
    "        inference_speed, output_shape, inference_peak_memory = measure_inference_metrics(model, input_tensor)\n",
    "        \n",
    "        print(f\"Measuring training metrics for {model_name}...\")\n",
    "        training_speed, training_peak_memory = measure_training_metrics(model, input_tensor)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'Inference': {\n",
    "                'Speed (s)': inference_speed,\n",
    "                'Output Shape': output_shape,\n",
    "                'Peak Memory Usage (MB)': inference_peak_memory\n",
    "            },\n",
    "            'Training': {\n",
    "                'Speed (s)': training_speed,\n",
    "                'Peak Memory Usage (MB)': training_peak_memory\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(f\"Inference - Speed: {inference_speed:.6f} seconds, Peak Memory: {inference_peak_memory:.2f} MB\")\n",
    "        print(f\"Training - Speed: {training_speed:.6f} seconds, Peak Memory: {training_peak_memory:.2f} MB\\n\")\n",
    "\n",
    "    print(\"All measurements completed.\")\n",
    "    return results\n",
    "\n",
    "# Call the main function to compare models\n",
    "model_prefix = \"dinov2_\"\n",
    "results = compare_models(model_prefix)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2cdbb-a344-45ec-814b-02de7f838364",
   "metadata": {},
   "source": [
    "## Training tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f32534c-e390-4fb9-ae89-4ce3b60763f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d28e89-38c6-44f4-9f95-8ef11491c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    train_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_train.csv',\n",
    "    val_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_val.csv',\n",
    "    test_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_test.csv',\n",
    "    model_embedder_weights='/lustre/fswork/projects/rech/ads/commun/models/resnet50/pretrained_weights.pt',\n",
    "    triplet_precompute_save_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/triplet_precompute.npz',\n",
    "    triplet_precompute_load_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/triplet_precompute.npz',\n",
    "    experiment_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full',\n",
    "    device='cuda',  # or 'cpu', 'auto'\n",
    "    verbose=True,\n",
    "    epochs=5,\n",
    "    debug=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea102f38-58c5-458a-99fa-6fac607047ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running train_triplets with arguments: Namespace(train_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_train.csv', val_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_val.csv', test_csv='/lustre/fsn1/projects/rech/ads/commun/datasets/balanced_noswiss_csv/lynx_id_balanced_test.csv', model_embedder_weights='/lustre/fswork/projects/rech/ads/commun/models/resnet50/pretrained_weights.pt', triplet_precompute_save_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/triplet_precompute.npz', triplet_precompute_load_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/triplet_precompute.npz', experiment_path='/lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full', device='cuda', verbose=True, epochs=5, debug=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torchvision-0.16.1+fdea156-py3.11-linux-x86_64.egg/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torchvision-0.16.1+fdea156-py3.11-linux-x86_64.egg/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8647\n",
      "torch.Size([8647, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 2059/2059 [00:00<00:00, 5143580.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_new=220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 2526/2526 [00:00<00:00, 5444404.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_new=279\n",
      "Model directory set to: /lustre/fswork/projects/idris/sos/commun/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fswork/projects/idris/sos/commun/models/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/lustre/fswork/projects/idris/sos/commun/models/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/lustre/fswork/projects/idris/sos/commun/models/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|##########| 1081/1081 [27:08<00:00,  1.51s/it, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.9540519634129051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|##########| 1081/1081 [27:05<00:00,  1.50s/it, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.912468336901656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|##########| 1081/1081 [27:06<00:00,  1.50s/it, loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.8953749380047734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|##########| 1081/1081 [26:57<00:00,  1.50s/it, loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.9104697824241275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|##########| 1081/1081 [27:03<00:00,  1.50s/it, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.8821458694396694\n",
      "Best model saved at: /lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/2024-10-21_13-50-30/model_best_0.882.pth\n",
      "Last model saved at: /lustre/fswork/projects/rech/ads/commun/kg_tests/experiments/dinov2l_noswiss_full/2024-10-21_13-50-30/model_last_0.882.pth\n",
      "Training completed. Now, start of evaluation on the model of the last epoch.\n"
     ]
    }
   ],
   "source": [
    "from lynx_id.scripts.train import train_dinov2\n",
    "train_dinov2.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aad8f-e5ad-40f8-a80d-a78bd075eebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d44b7-48b9-4712-8e35-7afe4b3fdfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.1.1_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
